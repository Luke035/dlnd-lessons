{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Net Preprocessing\n",
    "Notebook di processamento delle immagini di Image Net. Obiettivo è realizzare un batch input che, sfruttando il meccasnismo a code descritto in <a href=https://www.tensorflow.org/programmers_guide/reading_data>Tensorflow</a>, fornisca batch della dimensione desiderata per il numero di epoche desiderato.\n",
    "\n",
    "Viene inoltre sfruttanto l'algoritmo di <a href=https://github.com/tensorflow/models/blob/master/slim/preprocessing/inception_preprocessing.py>Inception preprocessing</a> per fornire in input immagini della dimensione corretta con le correzioni preaddestramento fornite da Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from PIL import Image\n",
    "#Inception preprocessing code from https://github.com/tensorflow/models/blob/master/slim/preprocessing/inception_preprocessing.py\n",
    "#useful to maintain training dimension\n",
    "from utils import inception_preprocessing\n",
    "import sys\n",
    "\n",
    "#from inception import inception\n",
    "'''\n",
    "Uso di slim e nets_factory (come per SLIM Tensorflow https://github.com/tensorflow/models/blob/master/slim/train_image_classifier.py)\n",
    "per il ripristino della rete. \n",
    "\n",
    "Le reti devono essere censite in nets_factory (v. struttura file nella directory di questo notebook)\n",
    "'''\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "from nets import nets_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Global Variables\n",
    "IMAGE_NET_ROOT_PATH = '/var/ifs/data/tiny-imagenet-200/'\n",
    "#IMAGE_NET_ROOT_PATH = '/data/lgrazioli/'\n",
    "IMAGE_NET_LABELS_PATH = IMAGE_NET_ROOT_PATH + 'words.txt'\n",
    "IMAGE_NET_TRAIN_PATH = IMAGE_NET_ROOT_PATH + 'train/'\n",
    "TRAINING_CHECKPOINT_DIR = '/tmp/ImageNetTrainTransfer'\n",
    "#Transfer learning CHECKPOINT PATH\n",
    "#File ckpt della rete\n",
    "CHECKPOINT_PATH = '/var/ifs/data/model-zoo/inceptionv4/tensorflow-1.2/inception_v4.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lettura file words di ImageNet\n",
    "Lettura del file words di ImageNet come PandaDF. A ogni id (cartella che contiene immagini per le classi fornite) vengono assegnati i label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/ipykernel/__main__.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n00001740</td>\n",
       "      <td>entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n00001930</td>\n",
       "      <td>physical entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n00002137</td>\n",
       "      <td>abstraction, abstract entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n00002452</td>\n",
       "      <td>thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n00002684</td>\n",
       "      <td>object, physical object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                        labels\n",
       "0  n00001740                        entity\n",
       "1  n00001930               physical entity\n",
       "2  n00002137  abstraction, abstract entity\n",
       "3  n00002452                         thing\n",
       "4  n00002684       object, physical object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading label file as Panda dataframe\n",
    "labels_df = pd.read_csv(IMAGE_NET_LABELS_PATH, sep='\\\\t', header=None, names=['id','labels'])\n",
    "labels_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        82115\n",
       "labels    82114\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggiunta colonna di lunghezza del label (quante classi contiene ogni label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#new_labels = []\n",
    "labels_lengths = []\n",
    "for idx, row in labels_df.iterrows():\n",
    "    #Convertire a stringa perchè alcuni sono float\n",
    "    current_labels = tuple(str(row['labels']).split(','))\n",
    "    #new_labels.append(current_labels)\n",
    "    labels_lengths.append(len(current_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_df['labels_length'] = labels_lengths\n",
    "labels_indices = [idx for idx, _ in labels_df.iterrows()]\n",
    "labels_df['indices'] = labels_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "      <th>labels_length</th>\n",
       "      <th>indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n00001740</td>\n",
       "      <td>entity</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n00001930</td>\n",
       "      <td>physical entity</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n00002137</td>\n",
       "      <td>abstraction, abstract entity</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n00002452</td>\n",
       "      <td>thing</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n00002684</td>\n",
       "      <td>object, physical object</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>n00003553</td>\n",
       "      <td>whole, unit</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>n00003993</td>\n",
       "      <td>congener</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>n00004258</td>\n",
       "      <td>living thing, animate thing</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>n00004475</td>\n",
       "      <td>organism, being</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>n00005787</td>\n",
       "      <td>benthos</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>n00005930</td>\n",
       "      <td>dwarf</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>n00006024</td>\n",
       "      <td>heterotroph</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>n00006150</td>\n",
       "      <td>parent</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>n00006269</td>\n",
       "      <td>life</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>n00006400</td>\n",
       "      <td>biont</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>n00006484</td>\n",
       "      <td>cell</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>n00007347</td>\n",
       "      <td>causal agent, cause, causal agency</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>n00007846</td>\n",
       "      <td>person, individual, someone, somebody, mortal,...</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>n00015388</td>\n",
       "      <td>animal, animate being, beast, brute, creature,...</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>n00017222</td>\n",
       "      <td>plant, flora, plant life</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             labels  \\\n",
       "0   n00001740                                             entity   \n",
       "1   n00001930                                    physical entity   \n",
       "2   n00002137                       abstraction, abstract entity   \n",
       "3   n00002452                                              thing   \n",
       "4   n00002684                            object, physical object   \n",
       "5   n00003553                                        whole, unit   \n",
       "6   n00003993                                           congener   \n",
       "7   n00004258                        living thing, animate thing   \n",
       "8   n00004475                                    organism, being   \n",
       "9   n00005787                                            benthos   \n",
       "10  n00005930                                              dwarf   \n",
       "11  n00006024                                        heterotroph   \n",
       "12  n00006150                                             parent   \n",
       "13  n00006269                                               life   \n",
       "14  n00006400                                              biont   \n",
       "15  n00006484                                               cell   \n",
       "16  n00007347                 causal agent, cause, causal agency   \n",
       "17  n00007846  person, individual, someone, somebody, mortal,...   \n",
       "18  n00015388  animal, animate being, beast, brute, creature,...   \n",
       "19  n00017222                           plant, flora, plant life   \n",
       "\n",
       "    labels_length  indices  \n",
       "0               1        0  \n",
       "1               1        1  \n",
       "2               2        2  \n",
       "3               1        3  \n",
       "4               2        4  \n",
       "5               2        5  \n",
       "6               1        6  \n",
       "7               2        7  \n",
       "8               2        8  \n",
       "9               1        9  \n",
       "10              1       10  \n",
       "11              1       11  \n",
       "12              1       12  \n",
       "13              1       13  \n",
       "14              1       14  \n",
       "15              1       15  \n",
       "16              3       16  \n",
       "17              6       17  \n",
       "18              6       18  \n",
       "19              3       19  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DF\n",
    "Panda Dataframe che contiene i path di tutte le immagini, la relativa classe, id dell'immagine e classe. La classe viene ottenuta tramite lookup su labels_df (<b>tale operazione pesa molto in termini di tempi di esecuzione</b>)\n",
    "\n",
    "<b>Può richiedere del tempo. Per lanciare su un campione si può bloccare a un determinato valore di idx</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing label n07747607\n",
      "Processing label n02917067\n",
      "Processing label n03400231\n",
      "Processing label n04179913\n",
      "Processing label n03837869\n",
      "Processing label n02074367\n",
      "im_path         3000\n",
      "class           3000\n",
      "im_class_id     3000\n",
      "target_label    3000\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>im_path</th>\n",
       "      <th>class</th>\n",
       "      <th>im_class_id</th>\n",
       "      <th>target_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/var/ifs/data/tiny-imagenet-200/train/n0774760...</td>\n",
       "      <td>n07747607</td>\n",
       "      <td>290</td>\n",
       "      <td>orange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/var/ifs/data/tiny-imagenet-200/train/n0774760...</td>\n",
       "      <td>n07747607</td>\n",
       "      <td>427</td>\n",
       "      <td>orange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/var/ifs/data/tiny-imagenet-200/train/n0774760...</td>\n",
       "      <td>n07747607</td>\n",
       "      <td>339</td>\n",
       "      <td>orange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/var/ifs/data/tiny-imagenet-200/train/n0774760...</td>\n",
       "      <td>n07747607</td>\n",
       "      <td>11</td>\n",
       "      <td>orange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/var/ifs/data/tiny-imagenet-200/train/n0774760...</td>\n",
       "      <td>n07747607</td>\n",
       "      <td>400</td>\n",
       "      <td>orange</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             im_path      class im_class_id  \\\n",
       "0  /var/ifs/data/tiny-imagenet-200/train/n0774760...  n07747607         290   \n",
       "1  /var/ifs/data/tiny-imagenet-200/train/n0774760...  n07747607         427   \n",
       "2  /var/ifs/data/tiny-imagenet-200/train/n0774760...  n07747607         339   \n",
       "3  /var/ifs/data/tiny-imagenet-200/train/n0774760...  n07747607          11   \n",
       "4  /var/ifs/data/tiny-imagenet-200/train/n0774760...  n07747607         400   \n",
       "\n",
       "  target_label  \n",
       "0       orange  \n",
       "1       orange  \n",
       "2       orange  \n",
       "3       orange  \n",
       "4       orange  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paths = []\n",
    "for idx, label_dir in enumerate(os.listdir(IMAGE_NET_TRAIN_PATH)):\n",
    "    image_dir_path = IMAGE_NET_TRAIN_PATH + label_dir + '/images/'\n",
    "    print(\"Processing label {0}\".format(label_dir))\n",
    "    for image in os.listdir(image_dir_path):\n",
    "        #Estrazione class_id\n",
    "        class_id = image.split('.')[0].split('_')[0]\n",
    "        #Lookup su labels df\n",
    "        target_label = labels_df[labels_df['id'] == class_id] #=> pass to tf.nn.one_hot\n",
    "        #Estrazione del label\n",
    "        target_label = target_label['labels'].values[0]\n",
    "        train_paths.append((image_dir_path + image, \n",
    "                            class_id,\n",
    "                            image.split('.')[0].split('_')[1],\n",
    "                            target_label\n",
    "                           ))\n",
    "    if idx == 5:\n",
    "        break\n",
    "train_df = pd.DataFrame(train_paths, columns=['im_path','class', 'im_class_id', 'target_label'])\n",
    "print(train_df.count())\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulizia delle immagini che non sono nel formato desiderato da inception_preprocessing (3 canali). \n",
    "<b>Operazione lunga!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2980 imagesNew size: 2946\n",
      "Removed 54 images\n"
     ]
    }
   ],
   "source": [
    "#Remove black and white images\n",
    "uncorrect_images = 0\n",
    "#Salvataggio indici di immagini da eliminare\n",
    "to_remove_indexes = []\n",
    "for idx, record in train_df.iterrows():\n",
    "    #Leggo immagine come np.array\n",
    "    im_array = np.array(Image.open(record['im_path']))\n",
    "    #Se non ha 3 canali la aggiungo a quelle da eliminare\n",
    "    if im_array.shape[-1] != 3:\n",
    "        uncorrect_images += 1\n",
    "        to_remove_indexes.append(idx)\n",
    "    if idx % 20 == 0:\n",
    "        sys.stdout.write(\"\\rProcessed {0} images\".format(idx))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "#Rimozione righe identificate\n",
    "train_df = train_df.drop(train_df.index[to_remove_indexes])\n",
    "\n",
    "print(\"New size: {0}\".format(len(train_df)))\n",
    "print(\"Removed {0} images\".format(uncorrect_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2946\n"
     ]
    }
   ],
   "source": [
    "#Eventuale campionamento da passare al generatore input\n",
    "example_file_list = list(train_df.im_path)\n",
    "print(len(example_file_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definizione dizionario dei labels\n",
    "{label: indice}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76003"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict = {}\n",
    "unique_labels = set(labels_df['labels'])\n",
    "for idx, target in enumerate(unique_labels):\n",
    "    labels_dict[target] = idx\n",
    "num_classes = len(labels_dict)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruzione lista dei label (stesso ordine della lista di file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2946"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_label_list = []\n",
    "for idx, value in train_df.iterrows():\n",
    "    example_label_list.append(labels_dict[value['target_label']])\n",
    "len(example_label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "Ripristino Inception v4 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "get_network_fn for returning the corresponding network function.\n",
    "\n",
    "Se num_classes è da cambiare, impostare is_training a True\n",
    "\n",
    "Ritorna la funzione definita nel corrispetivo file della rete\n",
    "'''\n",
    "model_name = 'inception_v4'\n",
    "inception_net_fn = nets_factory.get_network_fn(model_name,\n",
    "                                               num_classes=1001,\n",
    "                                               is_training = False\n",
    "                                              )\n",
    "with tf.device('/gpu:0'):\n",
    "    sampl_input = tf.placeholder(tf.float32, [None, 300,300, 3], name='incpetion_input_placeholder')\n",
    "    #Invocazione della model fn per la definizione delle variabili della rete\n",
    "    #Usa questi tensori che sono quelli per i quali passa il modello\n",
    "    #Necessario per ripristinare il grafo\n",
    "    inception_net_fn(sampl_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input pipeline\n",
    "Definizione della input pipeline al modello TF\n",
    "\n",
    "<b>NB: La memoria della GPU non va MAI oltre i 100MB!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 56\n",
    "#Serve per capire quando il generatore è passato a batch appartenenti a una nuova epoca \n",
    "BATCH_PER_EPOCH = np.ceil(len(example_file_list) / BATCH_SIZE)\n",
    "\n",
    "def parse_single_image(filename_queue):\n",
    "    #Dequeue a file name from the file name queue\n",
    "    #filename, y = filename_queue.dequeue()\n",
    "    #Non bisogna invocare il dequeue il parametro della funziona è già lo scodamento\n",
    "    filename, y = filename_queue[0], filename_queue[1]\n",
    "    #A y manca solo il one-hot\n",
    "    y = tf.one_hot(y, num_classes)\n",
    "    #Read image\n",
    "    raw = tf.read_file(filename)\n",
    "    #convert in jpg (in GPU!)\n",
    "    jpeg_image = tf.image.decode_jpeg(raw)\n",
    "    #Preprocessing with inception preprocessing\n",
    "    jpeg_image = inception_preprocessing.preprocess_image(jpeg_image, 300, 300, is_training=True)\n",
    "    return jpeg_image, y\n",
    "#jpeg_image = parse_single_image(filename_queue)\n",
    "\n",
    "def get_batch(filenames, labels, batch_size, num_epochs=None):\n",
    "    \n",
    "    #Coda lettura file, slice_input_producer accetta una lista di liste (stessa dimensione)\n",
    "    #Risultato dello scodamento è l'elemento corrente di ciascuna delle liste\n",
    "    #Le liste sono rispettivamente la lista di file e la lista dei label\n",
    "    filename_queue = tf.train.slice_input_producer([filenames, labels])\n",
    "    \n",
    "    #Lettura singolo record\n",
    "    jpeg_image,y = parse_single_image(filename_queue)\n",
    "    \n",
    "    # min_after_dequeue defines how big a buffer we will randomly sample\n",
    "    #   from -- bigger means better shuffling but slower start up and more\n",
    "    #   memory used.\n",
    "    # capacity must be larger than min_after_dequeue and the amount larger\n",
    "    #   determines the maximum we will prefetch.  Recommendation:\n",
    "    #   min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "    min_after_dequeue = 10\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    \n",
    "    #tensors è la lista dei tensori delle single feature e immagini. Esegue batch_size volte i tensori example e label per ottenere il batch\n",
    "    #num_threads incrementa effettivamente l'utilizzo della CPU (confermato dal throughput visisible sul cloudera manager,\n",
    "    #resta comunque un throughput lento ....\n",
    "    example_batch = tf.train.shuffle_batch(\n",
    "        tensors=[jpeg_image, y], batch_size=batch_size, capacity=capacity,\n",
    "        min_after_dequeue=min_after_dequeue, allow_smaller_final_batch=True, num_threads=2)\n",
    "    \n",
    "    return example_batch\n",
    "\n",
    "\n",
    "#TF Graph, per ora recupera solamente un batch\n",
    "with tf.device('/cpu:0'):\n",
    "    with tf.name_scope('preprocessing') as scope:\n",
    "        x,y = get_batch(example_file_list, example_label_list, batch_size=BATCH_SIZE)\n",
    "        #x = tf.contrib.layers.flatten(x)\n",
    "\n",
    "#inception prelogits \n",
    "#prelogits = tf.placeholder(tf.float32, [None, 1536], name='prelogits_placeholder')\n",
    "prelogits = tf.get_default_graph().get_tensor_by_name(\"InceptionV4/Logits/PreLogitsFlatten/Reshape:0\")        \n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    with tf.name_scope('hidden') as scope:\n",
    "        hidden = tf.layers.dense(\n",
    "            prelogits,\n",
    "            units=128,\n",
    "            activation=tf.nn.relu        \n",
    "        )\n",
    "\n",
    "    #Kenerl init None = glooroot initializers (sttdev = 1/sqrt(n))\n",
    "    with tf.name_scope('readout') as scope:\n",
    "        output = tf.layers.dense(\n",
    "            hidden,\n",
    "            units=num_classes,\n",
    "            activation=None\n",
    "        )\n",
    "\n",
    "    with tf.name_scope('train_op') as scope:\n",
    "        # Define loss and optimizer\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "\n",
    "    with tf.name_scope('train_metrics') as scope:\n",
    "        # Accuracy\n",
    "        correct_pred = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "tf.summary.scalar('loss', cost)\n",
    "\n",
    "init = tf. global_variables_initializer()\n",
    "\n",
    "merged_summeries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: preprocessing/input_producer/input_producer/input_producer_EnqueueMany = QueueEnqueueManyV2[Tcomponents=[DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](preprocessing/input_producer/input_producer, preprocessing/input_producer/input_producer/RandomShuffle)]]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[56,64,73,73]\n\t [[Node: InceptionV4/InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](InceptionV4/InceptionV4/Mixed_4a/Branch_1/Conv2d_0c_7x1/Relu, InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights/read)]]\n\nCaused by op 'InceptionV4/InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/convolution', defined at:\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-95e0d727faa1>\", line 18, in <module>\n    inception_net_fn(sampl_input)\n  File \"/data/lgrazioli/Transfer Learning/nets/nets_factory.py\", line 114, in network_fn\n    return func(images, num_classes, is_training=is_training)\n  File \"/data/lgrazioli/Transfer Learning/nets/inception_v4.py\", line 282, in inception_v4\n    net, end_points = inception_v4_base(inputs, scope=scope)\n  File \"/data/lgrazioli/Transfer Learning/nets/inception_v4.py\", line 209, in inception_v4_base\n    scope='Conv2d_1a_3x3')\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 947, in convolution\n    outputs = layer.apply(inputs)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 492, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 441, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/layers/convolutional.py\", line 158, in call\n    data_format=utils.convert_data_format(self.data_format, self.rank + 2))\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 670, in convolution\n    op=op)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 338, in with_space_to_batch\n    return op(input, num_spatial_dims, padding)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 662, in op\n    name=name)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 131, in _non_atrous_convolution\n    name=name)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 399, in conv2d\n    data_format=data_format, name=name)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[56,64,73,73]\n\t [[Node: InceptionV4/InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](InceptionV4/InceptionV4/Mixed_4a/Branch_1/Conv2d_0c_7x1/Relu, InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights/read)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[56,64,73,73]\n\t [[Node: InceptionV4/InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](InceptionV4/InceptionV4/Mixed_4a/Branch_1/Conv2d_0c_7x1/Relu, InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights/read)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8e185567d2d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#inception_pre_logits = sess.run(tf.get_default_graph().get_tensor_by_name(\"InceptionV4/Logits/PreLogitsFlatten/Reshape:0\"),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m          \u001b[0;31m#feed_dict={sampl_input: x_batch})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0msampl_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m#print(x_batch.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[56,64,73,73]\n\t [[Node: InceptionV4/InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](InceptionV4/InceptionV4/Mixed_4a/Branch_1/Conv2d_0c_7x1/Relu, InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights/read)]]\n\nCaused by op 'InceptionV4/InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/convolution', defined at:\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-95e0d727faa1>\", line 18, in <module>\n    inception_net_fn(sampl_input)\n  File \"/data/lgrazioli/Transfer Learning/nets/nets_factory.py\", line 114, in network_fn\n    return func(images, num_classes, is_training=is_training)\n  File \"/data/lgrazioli/Transfer Learning/nets/inception_v4.py\", line 282, in inception_v4\n    net, end_points = inception_v4_base(inputs, scope=scope)\n  File \"/data/lgrazioli/Transfer Learning/nets/inception_v4.py\", line 209, in inception_v4_base\n    scope='Conv2d_1a_3x3')\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 947, in convolution\n    outputs = layer.apply(inputs)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 492, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 441, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/layers/convolutional.py\", line 158, in call\n    data_format=utils.convert_data_format(self.data_format, self.rank + 2))\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 670, in convolution\n    op=op)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 338, in with_space_to_batch\n    return op(input, num_spatial_dims, padding)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 662, in op\n    name=name)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 131, in _non_atrous_convolution\n    name=name)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 399, in conv2d\n    data_format=data_format, name=name)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/opt/continuum/anaconda/envs/deepEnv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[56,64,73,73]\n\t [[Node: InceptionV4/InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](InceptionV4/InceptionV4/Mixed_4a/Branch_1/Conv2d_0c_7x1/Relu, InceptionV4/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights/read)]]\n"
     ]
    }
   ],
   "source": [
    "#GPU config\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "#Saver per restoring inception net\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter(TRAINING_CHECKPOINT_DIR,\n",
    "                                   sess.graph)\n",
    "    #Start populating the filename queue.\n",
    "    coord = tf.train.Coordinator()\n",
    "    #Senza questa chiamata non partono i thread per popolare la coda che permette di eseguire la read\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    #Current epoch and step servono a capire quando cambiare epoca e quando fermarsi\n",
    "    current_epoch = 0\n",
    "    current_step = 0\n",
    "    while current_epoch < EPOCHS: \n",
    "        x_batch, y_batch = sess.run([x,y])\n",
    "        #Forward pass nella incpetion net\n",
    "        #inception_pre_logits = sess.run(tf.get_default_graph().get_tensor_by_name(\"InceptionV4/Logits/PreLogitsFlatten/Reshape:0\"),\n",
    "         #feed_dict={sampl_input: x_batch})\n",
    "        sess.run(optimizer, feed_dict={sampl_input: x_batch, y: y_batch})\n",
    "        #print(x_batch.shape)\n",
    "        if current_step % 10 == 0:\n",
    "            #print(\"Batch shape {}\".format(x_batch.shape))\n",
    "            print(\"Current step: {0}\".format(current_step))\n",
    "            train_loss, train_accuracy, train_summ  = sess.run([cost,accuracy,merged_summeries],\n",
    "                                                               feed_dict={prelogits: inception_pre_logits, y: y_batch})\n",
    "            print(\"Loss: {0} accuracy {1}\".format(train_loss, train_accuracy))\n",
    "            writer.add_summary(train_summ, current_epoch * current_step + 1)\n",
    "        #Cambiare epoca, raggiunto il massimo per l'epoca corrente\n",
    "        if current_step == (BATCH_PER_EPOCH - 1):\n",
    "            current_epoch += 1\n",
    "            current_step = 0\n",
    "            print(\"EPOCH {0}\".format(current_epoch))\n",
    "        #Epoche terminate -> chiudere\n",
    "        if current_epoch >= EPOCHS:\n",
    "            break\n",
    "\n",
    "        if current_step == 0 and current_epoch == 0:\n",
    "            writer.add_graph(sess.graph)\n",
    "        #train_summary = sess.run([merged_summeries], feed_dict={x: x_batch, y: y_batch})\n",
    "        #writer.add_summary(train_summary, current_step)\n",
    "        current_step +=  1\n",
    "    #for i in range(10):\n",
    "        #converted_im = sess.run(jpeg_image)\n",
    "        #print(converted_im.shape)\n",
    "        \n",
    "    #Chiusura del coordinator (chiudi i thread di lettura)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
